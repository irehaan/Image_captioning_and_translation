# Image_captioning_and_translation

# Overview
I created this project to build an automated system capable of generating captions for images, answering questions based on images, and translating captions into multiple languages. Leveraging state-of-the-art deep learning models and web-based user interfaces, this engine aims to make image understanding and multilingual communication more accessible and convenient.
# Features
Automated Image Captioning: Generate descriptive captions for images using pre-trained models.Visual Question Answering (QnA): Ask questions about images and receive relevant answers.

Multilingual Translation: Translate generated captions into various languages to facilitate cross-cultural communication.
# Technologies Used
Python: Programming language used for development.
PyTorch: Deep learning framework for building and training neural networks.
Hugging Face Transformers: Library for accessing pre-trained models in natural language processing and computer vision.
Gradio: Library for creating web-based UIs for machine learning models.
PIL (Python Imaging Library): Library for image processing tasks.
SentencePiece: Tokenization library required for translation models.
ModelsBLIP Image Captioning: Pre-trained model for generating captions for images.
BLIP Visual QnA: Pre-trained model for answering questions based on images.
Translation Models: Pre-trained models for translating captions into various languages.
